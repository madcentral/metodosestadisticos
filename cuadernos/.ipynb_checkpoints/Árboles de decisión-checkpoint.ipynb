{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fda3ac8",
   "metadata": {},
   "source": [
    "<header style=\"background-color: #fff; position: fixed; width: 100%; z-index: 1; top: -10px;\"><p><br></p>\n",
    "           <p   style=\"color: #17318F;font-family:Lucida Sans Unicode, Lucida Grande, sans-serif;font-size:32px\" align=\"center\"> Métodos de clasificación supervisada - Árboles de decisión\n",
    " <a href='https://izainea.github.io'><img src=\"https://raw.githubusercontent.com/Izainea/Izainea.github.io/master/logos%20IZ-13.png\" alt=\"izainea\" style=\"margin: 10px;  float: right; width: 150px;\" ></a></p> \n",
    "<p><br></p>\n",
    "</header>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665d5f4",
   "metadata": {},
   "source": [
    "# ¿Qué son?\n",
    "Este es un tipo de algoritmo que progresivamente divide el conjunto de datos acordes a una característica descriptiva, lo hace hasta que cada conjunto es descrito por una etiqueta. Son muy utilizados en el ámbito del aprendizaje supervisado y de regresión, dependendiendo del tipo de la variable objetivo. Es quizás uno de los algoritmos de machine learning más usados dado que es uno de los algoritmos que brindan una fácil interpretación y resultan ser muy útiles en diferentes campos, como por ejemplo:\n",
    "\n",
    "- Chatbots\n",
    "- Análisis de sentimientos\n",
    "- Detección de fraudes\n",
    "- Entre otros...\n",
    "\n",
    "![img](../images/DecisionTree.png)\n",
    "\n",
    "El árbol se construye de arriba hacia abajo tratando de agrupar las observaciones similares entre sí y estableciendo reglas que máximicen la homogeneidad de los datos agrupados y heterogeneidad de los grupos diferentes. Cada hoja indica un tipo particular de datos, en el caso de variables continuas es útil particionar la variable por intervalos. El siguiente esquema explica como entender dichas particiones:\n",
    "\n",
    "Tomado de ([opendatascience.com](https://opendatascience.com/the-complete-guide-to-decision-trees-part-1/))\n",
    "\n",
    "<img loading=\"lazy\" class=\"progressiveMedia-image js-progressiveMedia-image alignnone\" src=\"https://cdn-images-1.medium.com/max/1000/0*cant-HQdfMju-GxG\" alt=\"\" width=\"500\" height=\"250\" data-src=\"https://cdn-images-1.medium.com/max/1000/0*cant-HQdfMju-GxG\">\n",
    "\n",
    "La complejidad de los árboles debe ser controlada, evitando problemas de sobreajuste y dificultades de interpretación, un método común que implementa el algoritmo para evitar este problema es la poda. Este proceso omite varios nodos que conforman ramas 'largas' de los árboles optimizando el rendimiento del árbol de decisión.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2d567",
   "metadata": {},
   "source": [
    "## Ventajas y desventajas\n",
    "\n",
    "<i class=\"fa fa-hand-peace-o\" style=\"font: sans;font-size:48px;color:blue\"> Ventajas</i>\n",
    "\n",
    "* Fácil de entender.\n",
    "* Util en exploración de datos:identificar importancia de variables a partir de cientos de variables.\n",
    "* Menos limpieza de datos: outliers y valores faltantes no influencian el modelo (A un cierto grado).\n",
    "* El tipo de datos no es una restricción.\n",
    "* Es un método no paramétrico (i.e., no hay suposición acerca del espacio de distribución y la estructura del clasificador).\n",
    "\n",
    "<i class=\"fa fa-hand-stop-o\" style=\"font:sans;font-size:48px;color:red\"> Desventajas</i>\n",
    "\n",
    "* Sobreajuste\n",
    "* Pérdida de información al categorizar variables continuas\n",
    "* Precisión: métodos como SVM y clasificadores tipo ensamblador a menudo tienen tasas de error 30% más bajas que CART (Classification and Regression Trees)\n",
    "* Inestabilidad: un pequeño cambio en los datos puede modificar ampliamente la estructura del árbol. Por lo tanto la interpretación no es tan directa como parece."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef093b4d",
   "metadata": {},
   "source": [
    "## Algoritmos usados en [sklearn](https://scikit-learn.org/stable/modules/tree.html)\n",
    "\n",
    "* ID3 (dicotomizador iterativo 3) fue desarrollado en 1986 por Ross Quinlan. El algoritmo crea un árbol de múltiples vías, encontrando para cada nodo (es decir, de manera codiciosa) la característica categórica que producirá la mayor ganancia de información para los objetivos categóricos. Los árboles crecen hasta su tamaño máximo y luego se suele aplicar un paso de poda para mejorar la capacidad del árbol de generalizar a datos invisibles.\n",
    "\n",
    "* C4.5 es el sucesor de ID3 y eliminó la restricción de que las características deben ser categóricas definiendo dinámicamente un atributo discreto (basado en variables numéricas) que divide el valor del atributo continuo en un conjunto discreto de intervalos. C4.5 convierte los árboles entrenados (es decir, la salida del algoritmo ID3) en conjuntos de reglas si-entonces. Luego, se evalúa la precisión de cada regla para determinar el orden en el que deben aplicarse. La poda se realiza eliminando la condición previa de una regla si la precisión de la regla mejora sin ella.\n",
    "\n",
    "* C5.0 es la última versión de Quinlan bajo licencia propietaria. Utiliza menos memoria y crea conjuntos de reglas más pequeños que C4.5 a la vez que es más preciso.\n",
    "\n",
    "* CART (árboles de clasificación y regresión) es muy similar a C4.5, pero se diferencia en que admite variables objetivo numéricas (regresión) y no calcula conjuntos de reglas. CART construye árboles binarios utilizando la característica y el umbral que producen la mayor ganancia de información en cada nodo.\n",
    "\n",
    "scikit-learn usa una versión optimizada del algoritmo CART; sin embargo, la implementación de scikit-learn no admite variables categóricas por ahora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7f968",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Los árboles de decisión permiten parámetyros como  min_samples_leaf y max_depth para prevenir un árbol sobreajustado, además el costo de complejidad de la poda proporciona otra opción para controlar el tamaño de un árbol. En `DecisionTreeClassifier` la complejidad hace referencia a ccp_alpha. Mayores valores de ccp_alpha aumentan el número de nudos podados. Aquí solo mostramos el efecto de ccp_alpha al regularizar los árboles y cómo elegir un ccp_alpha basado en puntajes de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb157b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80911caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "DF=pd.DataFrame(X,columns=load_breast_cancer()['feature_names'])\n",
    "DF['target']=y\n",
    "DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75363f76",
   "metadata": {},
   "source": [
    "## Impureza de hojas vs podas eficientes\n",
    "\n",
    "El costo mínimo de la complejidad de la poda aplica de forma recursiva el nodo con el \"eslabón más débil\". El eslabón más débil se caracteriza por un alfa efectivo, donde los nodos con el alfa efectivo más pequeño se podan primero. Para tener una idea de qué valores de ccp_alpha podrían ser apropiados, scikit-learn proporciona DecisionTreeClassifier.cost_complexity_pruning_path que devuelve los alfas efectivos y las impurezas totales de las hojas correspondientes en cada paso del proceso de poda. A medida que aumenta el alfa, se poda una mayor parte del árbol, lo que aumenta la impureza total de sus hojas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = plot_tree(clf, \n",
    "                   feature_names=load_breast_cancer()['feature_names'],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7c11a",
   "metadata": {},
   "source": [
    "En la siguiente gráfica, se elimina el valor alfa efectivo máximo, porque es el árbol trivial con un solo nodo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69462a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14693d9c",
   "metadata": {},
   "source": [
    "A continuación, entrenamos un árbol de decisiones utilizando los alfas efectivos. El último valor en ccp_alphases el valor alfa que poda todo el árbol, dejando el árbol clfs[-1], con un nodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d159b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "    fig = plt.figure(figsize=(25,20))\n",
    "    fig1 = plot_tree(clf, \n",
    "                   feature_names=load_breast_cancer()['feature_names'],\n",
    "                   filled=True)\n",
    "    \n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      clfs[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2d4f6",
   "metadata": {},
   "source": [
    "Para el resto de este ejemplo, eliminamos el último elemento en clfs y ccp_alphas, porque es el árbol trivial con un solo nodo. Aquí mostramos que el número de nodos y la profundidad del árbol disminuye a medida que aumenta el alfa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d26fb",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Cuando ccp_alpha se establece en cero y se mantienen los otros parámetros predeterminados de DecisionTreeClassifier, el árbol se sobreajusta, lo que lleva a una precisión de entrenamiento del 100% y una precisión de prueba del 88%. A medida que aumenta el alfa, se poda una mayor parte del árbol, creando así un árbol de decisiones que se generaliza mejor. En este ejemplo, la configuración ccp_alpha=0.015 maximiza la precisión de la prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
